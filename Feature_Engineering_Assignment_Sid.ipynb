{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a parameter ?**\n",
        "-  A parameter in machine learning and statistics is a configuration variable that is internal to the model and whose value can be estimated from the data. In algorithms like linear regression, parameters refer to coefficients that multiply input features (like the slope and intercept). These parameters are learned during training using optimization techniques that minimize the error between predicted and actual values. Parameters differ from hyperparameters, which are set before training and control the learning process itself. Understanding and adjusting parameters is crucial because they define the way the model generalizes from training data to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What is correlation ? What does negative correlation mean ?**\n",
        "-  Correlation measures the strength and direction of a linear relationship between two variables. A correlation coefficient ranges from -1 to 1. A positive correlation means that as one variable increases, the other tends to increase as well. A negative correlation means that as one variable increases, the other tends to decrease. For example, if the number of hours studied increases and the number of errors on a test decreases, the correlation is negative. Correlation is a statistical tool used to find relationships between variables, but it doesn’t imply causation—two variables may be correlated without one causing the other.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Define Machine Learning. What are the main components in Machine Learning ?**\n",
        "-  Machine Learning (ML) is a subset of artificial intelligence that involves training computer systems to learn patterns from data and make predictions or decisions without being explicitly programmed. The main components of ML are:\n",
        "1. **Data**: The foundation for learning.\n",
        "2. **Model**: The algorithm or function used to learn from data.\n",
        "3. **Training**: The process of learning model parameters from data.\n",
        "4. **Evaluation**: Assessing the model’s performance.\n",
        "5. **Prediction**: Applying the model to new, unseen data.\n",
        "6. **Loss Function**: Measures how far predictions are from actual values.\n",
        "7. **Optimizer**: Minimizes the loss function to improve model performance.\n",
        "\n",
        "---\n",
        "\n",
        "**4. How does loss value help in determining whether the model is good or not ?**\n",
        "-  The loss value is a numerical representation of the error between the model's predictions and the actual outcomes. A lower loss value typically indicates that the model’s predictions are close to the real values, implying a better model. Conversely, a high loss suggests that the model is underfitting or overfitting. Different types of loss functions exist, such as Mean Squared Error (MSE) for regression or Cross-Entropy Loss for classification. The goal during training is to minimize the loss. However, loss alone isn’t sufficient; it's important to monitor other metrics like accuracy, precision, and recall, especially in classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**5. What are continuous and categorical variables ?**  \n",
        "-  **Continuous variables** are numeric and can take an infinite number of values within a range. Examples include age, temperature, or height. They can be measured and are often used in regression problems.\n",
        "\n",
        "**Categorical variables**, on the other hand, represent categories or groups. They can be nominal (like gender or color) or ordinal (like education level. These variables don’t have a meaningful numerical value or order unless specified.\n",
        "\n",
        "In machine learning, we handle continuous and categorical data differently, especially during preprocessing. Understanding variable types is essential for choosing the right algorithms and preprocessing techniques.\n",
        "\n",
        "---\n",
        "\n",
        "**6. How do we handle categorical variables in Machine Learning? What are the common techniques ?**\n",
        "-  Handling categorical variables is crucial since most machine learning algorithms require numerical input. Common techniques include:\n",
        "  - **Label Encoding**: Converts categories into numbers (e.g., Red = 0, Blue = 1).\n",
        "  - **One-Hot Encoding**: Creates binary columns for each category, useful for nominal data.\n",
        "  - **Ordinal Encoding**: Assigns ordered values, best for ranked categories.\n",
        "  - **Binary Encoding**: Combines the benefits of one-hot and label encoding.\n",
        "  - **Target Encoding**: Replaces categories with the mean of the target variable.\n",
        "\n",
        "Choosing the right encoding method depends on the data and the model being used, as some models are sensitive to how categorical data is represented.\n",
        "\n",
        "---\n",
        "\n",
        "**7. What do you mean by training and testing a dataset ?**\n",
        "-  Training and testing a dataset refers to dividing the available data into two parts: one to train the model and the other to evaluate its performance. The training dataset is used by the machine learning algorithm to learn patterns and build the model. The testing dataset is used to assess how well the model generalizes to new, unseen data. This helps prevent overfitting, where the model performs well on training data but poorly on new data. Often, data is also split into a third set called the validation set, used for tuning hyperparameters before the final testing phase.\n",
        "\n",
        "---\n",
        "\n",
        "**8. What is sklearn.preprocessing ?**\n",
        "-  sklearn.preprocessing is a module in the Scikit-learn library that provides a set of tools for data preprocessing. These tools help transform raw data into a format suitable for training machine learning models. Functions include scaling features, encoding categorical variables, and normalizing data. For example, **StandardScaler** standardizes features by removing the mean and scaling to unit variance, and **OneHotEncoder** transforms categorical features into a one-hot numeric array. Preprocessing is a crucial step in the ML pipeline as it ensures that the data fits the assumptions of the chosen algorithm and improves model performance and convergence speed.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is a Test set ?**\n",
        "-  A test set is a subset of the dataset that is used to evaluate the final performance of a machine learning model. It contains data that the model has never seen during training or validation. The purpose of the test set is to simulate real-world data and assess how well the model generalizes. This helps determine the model's predictive power and its reliability when deployed in production. It’s important not to use the test set for tuning the model to avoid data leakage. Commonly, the data is split into 70% training and 30% test or other similar ratios.\n",
        "\n",
        "---\n",
        "\n",
        "**10. How do we split data for model fitting (training and testing) in Python ? How do you approach a Machine Learning problem ?**\n",
        "-  In Python, you can split data using train_test_split from sklearn.model_selection. For example:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "This splits the data into 80% training and 20% testing.\n",
        "\n",
        "Approach to a Machine Learning problem typically involves:\n",
        "1. Understanding the problem and data.\n",
        "2. Preprocessing data (cleaning, encoding, scaling).\n",
        "3. Exploratory Data Analysis (EDA).\n",
        "4. Choosing a suitable algorithm.\n",
        "5. Training the model.\n",
        "6. Evaluating using validation/testing data.\n",
        "7. Hyperparameter tuning.\n",
        "8. Final model deployment and monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "**11. Why do we have to perform EDA before fitting a model to the data ?**\n",
        "-  Exploratory Data Analysis (EDA) is crucial before model fitting because it helps you understand the underlying structure and patterns in the dataset. It involves visualizing and summarizing data distributions, identifying missing values, outliers, and anomalies, and discovering relationships between variables. EDA informs decisions about data cleaning, feature selection, and appropriate preprocessing techniques. It can reveal if features need scaling or encoding and whether the data is skewed. Without EDA, you risk training your model on flawed or misleading data, leading to poor performance and incorrect conclusions. Essentially, EDA ensures you’re not blindly feeding data into a model.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What is correlation ?** (Redundant Question)\n",
        "-  Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It is quantified using a correlation coefficient, usually Pearson’s r, which ranges from -1 to 1. A value of 1 indicates a perfect positive correlation, meaning as one variable increases, the other does too. A value of -1 indicates a perfect negative correlation, where one variable increases as the other decreases. A value near 0 implies no linear correlation. Correlation helps identify relationships in data, useful for feature selection and understanding data behavior, but it doesn’t imply causation.\n",
        "\n",
        "---\n",
        "\n",
        "**13. What does negative correlation mean ?** (Redundant Question)\n",
        "-  Negative correlation means that two variables move in opposite directions: when one increases, the other tends to decrease. The correlation coefficient for a negative relationship lies between 0 and -1. For example, if studying time increases while the number of mistakes on a test decreases, these two variables are negatively correlated. A perfect negative correlation (-1) means the variables are inversely related in a perfectly linear fashion. Negative correlations are important in data analysis for understanding trade-offs and inverse relationships. However, correlation does not establish cause and effect—just the direction and strength of the relationship.\n",
        "\n",
        "---\n",
        "\n",
        "**14. How can you find correlation between variables in Python ?**\n",
        "-  In Python, correlation can be computed using the .corr() method in pandas. This method calculates the Pearson correlation coefficient between all numerical columns in a DataFrame. For example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "correlation_matrix = df.corr()\n",
        "```\n",
        "\n",
        "To visualize correlation, you can use a heatmap from the seaborn library:\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "This makes it easy to detect strong correlations, both positive and negative, which can inform feature selection and model design.\n",
        "\n",
        "---\n",
        "\n",
        "**15. What is causation ? Explain the difference between correlation and causation with an example.**\n",
        "-  Causation implies that one event directly affects another. In contrast, correlation only shows that two variables are related in some way but doesn’t imply a direct cause-and-effect relationship.\n",
        "\n",
        "**Example**:\n",
        "If ice cream sales and drowning incidents both increase in summer, they are positively correlated. However, eating ice cream doesn’t cause drowning. The real cause is the warm weather, which leads to more people swimming and eating ice cream.\n",
        "\n",
        "This example shows how correlation can be misleading without understanding the context or underlying factors. Causal relationships require controlled experiments or deeper statistical analysis like randomized trials or regression modeling with confounders.\n",
        "\n",
        "---\n",
        "\n",
        "**16. What is an Optimizer ? What are different types of optimizers ? Explain each with an example.**\n",
        "-  An optimizer is an algorithm that adjusts model parameters (like weights) to minimize the loss function during training. It plays a key role in the learning process.\n",
        "\n",
        "**Types of optimizers**:\n",
        "  - **Gradient Descent**: Basic optimizer that updates parameters using the gradient of the loss function. Example: SGD.\n",
        "  - **Momentum**: Accelerates gradient descent by adding a fraction of the previous update.\n",
        "  - **Adagrad**: Adapts learning rates for each parameter, helpful for sparse data.\n",
        "  - **RMSprop**: Handles non-stationary objectives by keeping a moving average of squared gradients.\n",
        "  - **Adam**: Combines Momentum and RMSprop; widely used due to efficiency and adaptability.\n",
        "\n",
        "Example using Adam in TensorFlow:\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**17. What is sklearn.linear_model ?**\n",
        "-  sklearn.linear_model is a module in Scikit-learn that includes linear models for regression and classification. It provides tools like LinearRegression, LogisticRegression, Ridge, Lasso, and others.\n",
        "\n",
        "For example, LinearRegression() fits a linear model to data by minimizing the residual sum of squares between actual and predicted values.\n",
        "\n",
        "Example usage:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "This module is widely used because of its simplicity, interpretability, and effectiveness on linearly separable datasets. It also supports regularization techniques like L1 (Lasso) and L2 (Ridge) to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**18. What does model.fit() do ? What arguments must be given ?**\n",
        "-  The model.fit() method is used to train a machine learning model on a given dataset. It takes input features (X) and corresponding target values (y), then optimizes the model’s parameters using these examples.\n",
        "\n",
        "**Required arguments**:\n",
        "  - X_train: The training features (input data).\n",
        "  - y_train: The target variable (output labels).\n",
        "\n",
        "Optional arguments include epochs, batch_size, and validation_data for deep learning models.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "This is the core step where the model learns the relationships in data. After fitting, the model is ready to make predictions or be evaluated on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "**19. What does model.predict() do ? What arguments must be given ?**\n",
        "-  model.predict() is used to generate predictions from a trained model using new input data. It takes one argument—an array-like structure **X**, which contains the same number and type of features used during training.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "This function applies the learned model parameters (from fit()) to the input features and returns output values—either continuous (regression) or class probabilities/labels (classification). The result can be compared with the actual labels (y_test) to evaluate model performance using metrics like accuracy, precision, or RMSE.\n",
        "\n",
        "---\n",
        "\n",
        "**20. What are continuous and categorical variables ?** (Redundant Question)\n",
        "-  **Continuous variables** are numerical and can assume an infinite range of values, typically representing measurements. Examples include height, weight, age, and temperature. They support mathematical operations like addition and averaging.\n",
        "\n",
        "**Categorical variables** represent distinct groups or categories and are usually qualitative. Examples include gender, nationality, or blood type. These can be nominal (no order) or ordinal (ordered categories like low, medium, high).\n",
        "\n",
        "Machine learning models often require different preprocessing for these variable types—continuous variables might be scaled, while categorical variables need encoding. Distinguishing between these types is fundamental in data preparation.\n",
        "\n",
        "---\n",
        "\n",
        "**21. What is feature scaling ? How does it help in Machine Learning ?**\n",
        "-  Feature scaling is the process of normalizing the range of independent variables or features so they can be compared on the same scale. This is important for algorithms that rely on distance metrics (like KNN, SVM, or gradient descent optimization).\n",
        "\n",
        "Without scaling, features with larger values can disproportionately influence the model.\n",
        "\n",
        "Common scaling techniques include:\n",
        "  - **Standardization (Z-score)**: Centers data to have mean = 0 and std = 1.\n",
        "  - **Min-Max Scaling**: Rescales data to a [0, 1] range.\n",
        "\n",
        "Scaling improves convergence speed, stability, and accuracy of models. It’s a crucial preprocessing step in nearly every ML pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "**22. How do we perform scaling in Python ?**\n",
        "-  Scaling in Python is typically done using Scikit-learn’s sklearn.preprocessing module.\n",
        "\n",
        "For example, to use StandardScaler:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "For Min-Max Scaling:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "These scalers fit on training data and transform both training and test data accordingly. It’s important to fit the scaler only on the training set to avoid data leakage and apply the same transformation to the test set.\n",
        "\n",
        "---\n",
        "\n",
        "**23. What is sklearn.preprocessing ?** (Redundant Question)\n",
        "-  sklearn.preprocessing is a module in Scikit-learn that provides utility functions and classes for transforming raw data into a form suitable for model training. It includes tools for:\n",
        "    - **Feature scaling** (StandardScaler, MinMaxScaler)\n",
        "    - **Encoding** (LabelEncoder, OneHotEncoder)\n",
        "    - **Normalization** (Normalizer)\n",
        "    - **Imputation** for missing values\n",
        "\n",
        "These tools are essential for data preparation and ensure that machine learning models receive input data in the correct format and scale. Without proper preprocessing, models may perform poorly or fail to converge.\n",
        "\n",
        "---\n",
        "\n",
        "**24. How do we split data for model fitting (training and testing) in Python ?** (Redundant Question)\n",
        "-  In Python, the Scikit-learn library provides the train_test_split() function to split data into training and testing sets.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train\n",
        "\n",
        "_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "The test_size defines the percentage of data for testing, and random_state ensures reproducibility. You can also use stratify=y to maintain the distribution of classes in classification problems. Properly splitting data helps evaluate the model’s ability to generalize to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "**25. Explain data encoding.**\n",
        "-  Data encoding is the process of converting categorical variables into a numerical format that machine learning models can understand. Since most algorithms work only with numerical inputs, encoding is essential.\n",
        "\n",
        "**Common techniques include**:  \n",
        "  - **Label Encoding**: Assigns a unique integer to each category.\n",
        "  - **One-Hot Encoding**: Creates binary columns for each category.\n",
        "  - **Ordinal Encoding**: Maps ordered categories to integers.\n",
        "  - **Target Encoding**: Replaces categories with the mean target value.\n",
        "\n",
        "Encoding helps transform non-numeric features like gender or country into usable inputs. Choosing the right encoding method is important, as poor encoding can lead to model bias or performance issues."
      ],
      "metadata": {
        "id": "ztU8QPSSeqRq"
      }
    }
  ]
}
